var settings = {
  "modelName": "lda100",
  "numTopics": 5,
  "optimize": {
    "steps": 1, // 10000
    "optMethod": {
      "adam": {
        "stepSize": 0.01,
        "decayRate1": 0.9,
        "decayRate2": 0.999
      }
    },
    "estimator": {
      "ELBO": {
        "samples": 1,
        "avgBaselines": true
      }
    }
  }
};


// --------------------------------------------------------------------
// Utils

var forEach = function(xs, f) {
  map(function(i) {
    return f(xs[i], i);
  }, _.range(xs.length));
  return;
};


// --------------------------------------------------------------------
// Models

var lda100 = function(data, vocabSize, numTopics, alpha, eta) {
  var corpus = data.documentsAsCounts;

  var topics = repeat(numTopics, function() {
    return sample(Dirichlet({alpha: eta}));
  });

  mapData({data: corpus}, function(doc) {

    var topicDist = sample(Dirichlet({alpha: alpha}));

    forEach(doc, function(count, word) {

      // Naive summing out.
      // if (count > 0) {
      //   var marginal = Enumerate(function() {
      //     var z = sample(Discrete({ps: topicDist}));
      //     var topic = topics[z];
      //     return sample(Discrete({ps: topic}));
      //   });

      //   factor(count * marginal.score(word));
      // }

      // More efficient summing out of z by not "sampling" w.

      // What I think we might want here is something which like
      // Enumerate that explores all paths (through a thunk), but
      // rather than building a histogram it just returns the (log of)
      // sum of the probability of each path?

      if (count > 0) {
        // Sum over topic assignments/z.
        var prob = sum(mapN(function(z) {
          // In this particular model we could just index into
          // topicDist/topics[z] to get at the log probability.
          var zScore = Discrete({ps: topicDist}).score(z);
          var wgivenzScore = Discrete({ps: topics[z]}).score(word);
          return Math.exp(zScore + wgivenzScore);
        }, numTopics));

        factor(Math.log(prob) * count);
      }

    });

  });

  return topics;

};


// --------------------------------------------------------------------

var models = {
  lda100: lda100
};

var modelName = settings.modelName;
assert.ok(_.has(models, modelName), 'Model not found.');

var model = function(data) {
  var f = models[modelName];
  var vocabSize = data.numWords;  // V
  var numTopics = settings.numTopics;  // K
  var alpha = T.add(zeros([numTopics, 1]), 0.1);  // Parameter for prior on topic proportions.
  var eta = T.add(zeros([vocabSize, 1]), 0.1);  // Parameter for prior on topics.  
  return f(data, vocabSize, numTopics, alpha, eta);
};
